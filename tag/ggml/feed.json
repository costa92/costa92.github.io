{
    "version": "https://jsonfeed.org/version/1",
    "title": "翼舞成梦",
    "subtitle": "翼舞成梦",
    "icon": "https://costalong.com/images/favicon.ico",
    "description": "",
    "home_page_url": "https://costalong.com",
    "items": [
        {
            "id": "https://costalong.com/2026/01/03/llama/llama-install/",
            "url": "https://costalong.com/2026/01/03/llama/llama-install/",
            "title": "llama 安装",
            "date_published": "2026-01-03T15:36:50.211Z",
            "content_html": "<h2 id=\"安装环境\"><a href=\"#安装环境\" class=\"headerlink\" title=\"安装环境\"></a>安装环境</h2><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> add-apt-repository ppa:deadsnakes/ppa\n<span class=\"token function\">sudo</span> apt update\n<span class=\"token function\">sudo</span> apt <span class=\"token function\">install</span> python3.11 python3.11-venv -y\n</code></pre>\n<p>安装虚拟环境</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">/usr/bin/python3.11 -m venv venv\n</code></pre>\n<p>使用虚拟</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">source</span> venv/bin/activate\n<span class=\"token comment\" spellcheck=\"true\"># 如果使用的 fish</span>\n<span class=\"token function\">source</span> venv/bin/activate.fish\n</code></pre>\n<p>安装 Install PyTorch:</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"> pip <span class=\"token function\">install</span> --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n</code></pre>\n<h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><p>1. 下载 llama.cp</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">git</span> clone https://github.com/ggerganov/llama.cpp.git \n</code></pre>\n<ol>\n<li>下载通义千问1.5-7B模型 或 1.5-32B 模型&amp;#x20;</li>\n</ol>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token comment\" spellcheck=\"true\"># 使用 huggingface 仓库</span>\n<span class=\"token function\">git</span> clone https://huggingface.co/Qwen/Qwen1.5-7B-Chat\n<span class=\"token comment\" spellcheck=\"true\"># 或</span>\n<span class=\"token function\">git</span> clone https://huggingface.co/qwen/Qwen1.5-32B-Chat.git\n\n\n<span class=\"token comment\" spellcheck=\"true\"># 使用 modelscope 仓库</span>\n<span class=\"token function\">git</span> clone https://www.modelscope.cn/Qwen/Qwen1.5-7B-Chat\n<span class=\"token comment\" spellcheck=\"true\"># 或</span>\n<span class=\"token function\">git</span> clone https://www.modelscope.cn/qwen/Qwen1.5-32B-Chat.git\n</code></pre>\n<h2 id=\"编译-llama-cp\"><a href=\"#编译-llama-cp\" class=\"headerlink\" title=\"编译 llama.cp\"></a>编译 llama.cp</h2><pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">cd</span> llama.cpp \n<span class=\"token function\">make</span>\n</code></pre>\n<h3 id=\"安装llama-依赖\"><a href=\"#安装llama-依赖\" class=\"headerlink\" title=\"安装llama 依赖\"></a>安装llama 依赖</h3><pre class=\" language-bash\"><code class=\"language-bash\">python3 -m  pip <span class=\"token function\">install</span> -r requirements.txt\n</code></pre>\n<h3 id=\"转换-Qwen-模型为-GGUF\"><a href=\"#转换-Qwen-模型为-GGUF\" class=\"headerlink\" title=\"转换 Qwen 模型为 GGUF\"></a>转换 Qwen 模型为 GGUF</h3><p>什么是 GGUF? GGUF是一种用于存储用于GGML推断和基于GGML的执行器的模型的文件格式。GGUF是一种二进制格式，旨在快速加载和保存模型，并易于阅读。传统上，模型是使用PyTorch或其他框架开发的，然后转换为GGUF以在GGML中使用。</p>\n<p>GGUF是GGML、GGMF和GGJT的后继文件格式，旨在通过包含加载模型所需的所有信息来消除歧义。它还设计为可扩展的，因此可以向模型添加新信息而不会破坏兼容性，更多信息访问<a href=\"https://github.com/ggerganov/ggml/blob/master/docs/gguf.md\" title=\"官方说明文档\">官方说明文档</a>。</p>\n<p>查看文件结构</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">$ tree -L 1\n<span class=\"token keyword\">.</span>\n├── llama.cpp  \n├── Qwen1.5-32B-Chat\n└── venv\n</code></pre>\n<p>执行这转换命令</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">python3 convert-hf-to-gguf.py  <span class=\"token punctuation\">..</span>/Qwen1.5-32B-Chat\n\n<span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token punctuation\">..</span><span class=\"token punctuation\">..</span>.\nINFO:hf-to-gguf:Model successfully exported to <span class=\"token string\">'../Qwen1.5-32B-Chat/ggml-model-f16.gguf'</span>\n</code></pre>\n<p>最后的结果表示已经转为 F16 的 gguf 格式的模型了</p>\n<h3 id=\"量化模型\"><a href=\"#量化模型\" class=\"headerlink\" title=\"量化模型\"></a>量化模型</h3><p>将gguf 的模型量化到INT4&amp;#x20;</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">./llama-quantize  <span class=\"token punctuation\">..</span>/Qwen1.5-32B-Chat/ggml-model-f16.gguf  ./models/qwen1.5-chat-ggml-model-Q4_K_M.gguf Q4_K_M\n<span class=\"token operator\">=</span><span class=\"token operator\">></span>  <span class=\"token punctuation\">..</span><span class=\"token punctuation\">..</span>\n<span class=\"token punctuation\">[</span> 767/ 771<span class=\"token punctuation\">]</span>                   blk.63.attn_q.bias - <span class=\"token punctuation\">[</span> 5120,     1,     1,     1<span class=\"token punctuation\">]</span>, <span class=\"token function\">type</span> <span class=\"token operator\">=</span>    f32, size <span class=\"token operator\">=</span>    0.020 MB\n<span class=\"token punctuation\">[</span> 768/ 771<span class=\"token punctuation\">]</span>                 blk.63.attn_q.weight - <span class=\"token punctuation\">[</span> 5120,  5120,     1,     1<span class=\"token punctuation\">]</span>, <span class=\"token function\">type</span> <span class=\"token operator\">=</span>    f16, converting to q4_K <span class=\"token punctuation\">..</span> size <span class=\"token operator\">=</span>    50.00 MiB -<span class=\"token operator\">></span>    14.06 MiB\n<span class=\"token punctuation\">[</span> 769/ 771<span class=\"token punctuation\">]</span>                   blk.63.attn_v.bias - <span class=\"token punctuation\">[</span> 1024,     1,     1,     1<span class=\"token punctuation\">]</span>, <span class=\"token function\">type</span> <span class=\"token operator\">=</span>    f32, size <span class=\"token operator\">=</span>    0.004 MB\n<span class=\"token punctuation\">[</span> 770/ 771<span class=\"token punctuation\">]</span>                 blk.63.attn_v.weight - <span class=\"token punctuation\">[</span> 5120,  1024,     1,     1<span class=\"token punctuation\">]</span>, <span class=\"token function\">type</span> <span class=\"token operator\">=</span>    f16, converting to q6_K <span class=\"token punctuation\">..</span> size <span class=\"token operator\">=</span>    10.00 MiB -<span class=\"token operator\">></span>     4.10 MiB\n<span class=\"token punctuation\">[</span> 771/ 771<span class=\"token punctuation\">]</span>                   output_norm.weight - <span class=\"token punctuation\">[</span> 5120,     1,     1,     1<span class=\"token punctuation\">]</span>, <span class=\"token function\">type</span> <span class=\"token operator\">=</span>    f32, size <span class=\"token operator\">=</span>    0.020 MB\nllama_model_quantize_internal: model size  <span class=\"token operator\">=</span> 62014.27 MB\nllama_model_quantize_internal: quant size  <span class=\"token operator\">=</span> 18780.70 MB\n\nmain: quantize <span class=\"token function\">time</span> <span class=\"token operator\">=</span> 421046.36 ms\nmain:    total <span class=\"token function\">time</span> <span class=\"token operator\">=</span> 421046.36 ms\n\n\n</code></pre>\n<p>../Qwen1.5-32B-Chat/ggml-model-f16.gguf   是转换的源文件路径</p>\n<p>./models/qwen1.5-chat-ggml-model-Q4_K_M.gguf   是转换成功生成的文件路径</p>\n<p>Q4_K_M 是量化方法</p>\n<p>新版量化方法包括：</p>\n<ul>\n<li>Q2_K</li>\n<li>Q3_K_S, Q3_K_M, Q3_K_L</li>\n<li>Q4_K_S, Q4_K_M</li>\n<li>Q5_K_S, Q5_K_M</li>\n<li>Q6_K</li>\n</ul>\n<h3 id=\"运行\"><a href=\"#运行\" class=\"headerlink\" title=\"运行\"></a>运行</h3><h3 id=\"运行测试\"><a href=\"#运行测试\" class=\"headerlink\" title=\"运行测试\"></a>运行测试</h3><pre class=\" language-bash\"><code class=\"language-bash\">./llama-cli -m ./models/qwen1.5-chat-ggml-model-Q4_K_M.gguf -n 128\n<span class=\"token operator\">=</span><span class=\"token operator\">></span> \n\n1<span class=\"token punctuation\">)</span> 甲乙丙丁四地中，最可能出现水土流失的是\nA. 甲\nB0 乙\nC0 丙\nD0 丁\n答案：C\n关键点：中国分区地理，水土流失及治理，区域农业生产的条件、布局特点、问题\n\n\nllama_print_timings:        load <span class=\"token function\">time</span> <span class=\"token operator\">=</span>   17027.80 ms\nllama_print_timings:      sample <span class=\"token function\">time</span> <span class=\"token operator\">=</span>      22.98 ms /   128 runs   <span class=\"token punctuation\">(</span>    0.18 ms per token,  5571.27 tokens per second<span class=\"token punctuation\">)</span>\nllama_print_timings: prompt <span class=\"token function\">eval</span> <span class=\"token function\">time</span> <span class=\"token operator\">=</span>       0.00 ms /     0 tokens <span class=\"token punctuation\">(</span>    -nan ms per token,     -nan tokens per second<span class=\"token punctuation\">)</span>\nllama_print_timings:        <span class=\"token function\">eval</span> <span class=\"token function\">time</span> <span class=\"token operator\">=</span>  867086.12 ms /   128 runs   <span class=\"token punctuation\">(</span> 6774.11 ms per token,     0.15 tokens per second<span class=\"token punctuation\">)</span>\nllama_print_timings:       total <span class=\"token function\">time</span> <span class=\"token operator\">=</span>  867381.32 ms /   128 tokens\nLog end\n</code></pre>\n<p>注意： 这个运行等待的时间比较长</p>\n<p>接下来我们进入对话模型。如何启动呢？我们这里查看 examples/chat.sh的启动方式，来编写启动 Qwen 模型命令。</p>\n<pre class=\" language-bash\"><code class=\"language-bash\">./llama-cli -m ./models/qwen1.5-chat-ggml-model-Q4_K_M.gguf  -n 256 --grammar-file grammars/json.gbnf -p <span class=\"token string\">'Request: schedule a call at 8pm; Command:'</span>\n</code></pre>\n<h2 id=\"错误\"><a href=\"#错误\" class=\"headerlink\" title=\"错误\"></a>错误</h2><ol>\n<li>通义千问safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge解决方法</li>\n</ol>\n<p>安装 gi-lfs</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token function\">install</span> git-lfs\n</code></pre>\n<p>进入 Qwen1.5-32B-Chat  执行 git lfs pull</p>\n<pre class=\" language-bash\"><code class=\"language-bash\"><span class=\"token function\">cd</span> Qwen1.5-32B-Chat <span class=\"token operator\">&amp;&amp;</span> <span class=\"token function\">git</span> lfs pull\n</code></pre>\n<link rel=\"stylesheet\" href=\"/css/spoiler.css\" type=\"text/css\"><script src=\"/js/spoiler.js\" type=\"text/javascript\" async></script>",
            "tags": [
                "llama",
                "ggml"
            ]
        }
    ]
}